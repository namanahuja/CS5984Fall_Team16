{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "import json\n",
    "import io\n",
    "import shutil\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mapping.txt and run the command manually\n",
    "def tokenize_stories(stories_dir, tokenized_stories_dir):\n",
    "    \"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n",
    "    print \"Preparing to tokenize %s to %s...\" % (stories_dir, tokenized_stories_dir)\n",
    "    stories = os.listdir(stories_dir)\n",
    "    # make IO list file\n",
    "    print \"Making list of files to tokenize...\"\n",
    "    if not os.path.isdir(separated_dir):\n",
    "        os.mkdir(separated_dir)\n",
    "    with open(\"mapping.txt\", \"w\") as f:\n",
    "        for s in stories:\n",
    "            f.write(\"%s \\t %s\\n\" % (os.path.join(stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
    "    # call system command\n",
    "    #os.system(\"java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines /home/xw0078/jupyter_notebooks/ETD/mapping.txt\") \n",
    "    mapping_path = \"/home/xw0078/jupyter_notebooks/ETD/mapping.txt\"\n",
    "    os.environ['CLASSPATH'] = \"/home/xw0078/my_programs/lib/stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar\"\n",
    "    #command_1 = \"export CLASSPATH=/home/xw0078/my_programs/lib/stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar\"\n",
    "    command_2 = \"java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines /home/xw0078/jupyter_notebooks/ETD/mapping.txt\"\n",
    "    #subprocess.check_output(command_1,shell=True)\n",
    "    subprocess.check_output(command_2,shell=True)\n",
    "    os.remove(\"mapping.txt\")\n",
    "    print \"Tokenization Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and separate Json file to abstract and body text\n",
    "def separate_json(input_path, separated_dir):\n",
    "    # Open the file with read only permit\n",
    "    jsonFile = io.open(input_path,'r', encoding='utf8')\n",
    "    # use readline() to read the first line \n",
    "    line = jsonFile.readline()\n",
    "    # use the read line to read further.\n",
    "    # If the file is not empty keep reading one line\n",
    "    # at a time, till the file is empty\n",
    "    ID = 0\n",
    "    flag = 0\n",
    "    empty_count = 0\n",
    "    while line:\n",
    "        record = json.loads(line)\n",
    "        abstract =  record['introduction']\n",
    "        body = record['textbody']\n",
    "        if len(abstract) < 50:\n",
    "            line = jsonFile.readline()\n",
    "            continue\n",
    "        if len(body) < 100:\n",
    "            line = jsonFile.readline()\n",
    "            continue\n",
    "        #write abstract\n",
    "        if not os.path.isdir(separated_dir):\n",
    "            os.mkdir(separated_dir)\n",
    "        f = open(separated_dir+str(ID)+\".abs\",\"wb\")\n",
    "        f.write(abstract.encode(\"utf-8\",'ignore'))\n",
    "        f.close()\n",
    "        #write bodytext\n",
    "        f = open(separated_dir+str(ID)+\".body\",\"wb\")\n",
    "        f.write(body.encode(\"utf-8\",'ignore'))\n",
    "        f.close()\n",
    "\n",
    "        #next line\n",
    "        ID = ID+1\n",
    "        line = jsonFile.readline()\n",
    "    jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/mnt/6t/wiki/processed/ETD_related_abstract_v4.json\"\n",
    "separated_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/separated_files/\"\n",
    "\n",
    "shutil.rmtree(separated_dir)\n",
    "os.mkdir(separated_dir)\n",
    "separate_json(input_path,separated_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to tokenize /mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/separated_files/ to /mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/tokenized_separated_files/...\n",
      "Making list of files to tokenize...\n",
      "Tokenization Finished\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/separated_files/\"\n",
    "tokenized_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/tokenized_separated_files/\"\n",
    "\n",
    "shutil.rmtree(tokenized_dir)\n",
    "os.mkdir(tokenized_dir)\n",
    "tokenize_stories(input_dir,tokenized_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_art_abs(input_dir,ID):\n",
    "    with open(input_dir+ID+\".abs\", 'r') as absFile:\n",
    "        abstract = absFile.read()\n",
    "    with open(input_dir+ID+\".body\", 'r') as bodyFile:\n",
    "        body = bodyFile.read()\n",
    "    return body,abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to bin file\n",
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
    "VOCAB_SIZE = 200000\n",
    "# We use these to separate the summary sentences in the .bin datafiles\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "def write_to_bin(input_dir, out_file, makevocab=False):\n",
    "    \"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n",
    "    if makevocab:\n",
    "        vocab_counter = collections.Counter()\n",
    "    ID = 0\n",
    "    with open(out_file, 'wb') as writer:\n",
    "        # Get the strings to write to .bin file\n",
    "        num_files = len(os.listdir(input_dir))/2-1\n",
    "        for ID in range(0,num_files):\n",
    "            \n",
    "            article, abstract = get_art_abs(input_dir,str(ID))\n",
    "\n",
    "            # Write to tf.Example\n",
    "            tf_example = example_pb2.Example()\n",
    "            tf_example.features.feature['article'].bytes_list.value.extend([article])\n",
    "            tf_example.features.feature['abstract'].bytes_list.value.extend([abstract])\n",
    "            tf_example_str = tf_example.SerializeToString()\n",
    "            str_len = len(tf_example_str)\n",
    "            if not str_len:\n",
    "                print \"MyError-0\"\n",
    "            writer.write(struct.pack('q', str_len))\n",
    "            writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "\n",
    "            # Write the vocab to file, if applicable\n",
    "            if makevocab:\n",
    "                art_tokens = article.split(' ')\n",
    "                abs_tokens = abstract.split(' ')\n",
    "                abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n",
    "                tokens = art_tokens + abs_tokens\n",
    "                tokens = [t.strip() for t in tokens] # strip\n",
    "                tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "                vocab_counter.update(tokens)\n",
    "\n",
    "    print \"Finished writing file %s\\n\" % out_file\n",
    "\n",
    "    # write vocab to file\n",
    "    if makevocab:\n",
    "        print \"Writing vocab file...\"\n",
    "        with open(os.path.join(\"/mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/finished_files/\", \"vocab_wikiETD\"), 'w') as writer:\n",
    "            for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "                writer.write(word + ' ' + str(count) + '\\n')\n",
    "        print \"Finished writing vocab file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing file /mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/finished_files/train.bin\n",
      "\n",
      "Writing vocab file...\n",
      "Finished writing vocab file\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/tokenized_separated_files/\"\n",
    "output_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/finished_files/train.bin\"\n",
    "write_to_bin(input_dir,output_dir,makevocab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/finished_files/chunked/\"\n",
    "CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n",
    "def chunk_file(set_name):\n",
    "    in_file = '/mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/finished_files/%s.bin' % set_name\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        chunk_fname = os.path.join(chunks_dir, '%s_wikiETD_%03d.bin' % (set_name, chunk)) # new chunk\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "        \n",
    "def chunk_all():\n",
    "  # Make a dir to hold the chunks\n",
    "    if not os.path.isdir(chunks_dir):\n",
    "        os.mkdir(chunks_dir)\n",
    "  # Chunk the data\n",
    "    for set_name in ['train']:\n",
    "        print \"Splitting %s data into chunks...\" % set_name\n",
    "        chunk_file(set_name)\n",
    "    print \"Saved chunked data in %s\" % chunks_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train data into chunks...\n",
      "Saved chunked data in /mnt/6t/DeepLearningPreTrainModels/wiki_etd_data/finished_files/chunked/\n"
     ]
    }
   ],
   "source": [
    "chunk_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch03-p27]",
   "language": "python",
   "name": "conda-env-pytorch03-p27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
