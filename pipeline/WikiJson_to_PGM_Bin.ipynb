{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from tensorflow.core.example import example_pb2\n",
    "import json\n",
    "import io\n",
    "import shutil\n",
    "import subprocess\n",
    "import pathlib\n",
    "from nltk import sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate mapping.txt and run the command manually\n",
    "def tokenize_stories(stories_dir, tokenized_stories_dir):\n",
    "    # clear data directory\n",
    "    if os.path.exists(tokenized_dir):\n",
    "        shutil.rmtree(tokenized_dir)\n",
    "        os.makedirs(tokenized_dir)\n",
    "    else:\n",
    "        os.makedirs(tokenized_dir)\n",
    "    \n",
    "    \"\"\"Maps a whole directory of .story files to a tokenized version using Stanford CoreNLP Tokenizer\"\"\"\n",
    "    print \"Preparing to tokenize %s to %s...\" % (stories_dir, tokenized_stories_dir)\n",
    "    stories = os.listdir(stories_dir)\n",
    "    # make IO list file\n",
    "    print \"Making list of files to tokenize...\"\n",
    "    if not os.path.isdir(separated_dir):\n",
    "        os.mkdir(separated_dir)\n",
    "    with open(\"mapping.txt\", \"w\") as f:\n",
    "        for s in stories:\n",
    "            f.write(\"%s \\t %s\\n\" % (os.path.join(stories_dir, s), os.path.join(tokenized_stories_dir, s)))\n",
    "    # call system command\n",
    "    #os.system(\"java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines /home/xw0078/jupyter_notebooks/ETD/mapping.txt\") \n",
    "    mapping_path = \"/home/xw0078/jupyter_notebooks/ETD/mapping.txt\"\n",
    "    os.environ['CLASSPATH'] = \"/home/xw0078/my_programs/lib/stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar\"\n",
    "    #command_1 = \"export CLASSPATH=/home/xw0078/my_programs/lib/stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar\"\n",
    "    command_2 = \"java edu.stanford.nlp.process.PTBTokenizer -ioFileList -preserveLines /home/xw0078/jupyter_notebooks/ETD/mapping.txt\"\n",
    "    #subprocess.check_output(command_1,shell=True)\n",
    "    subprocess.check_output(command_2,shell=True)\n",
    "    os.remove(\"mapping.txt\")\n",
    "    print \"Tokenization Finished\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and separate Json file to abstract and body text\n",
    "def separate_json(input_path, separated_dir):\n",
    "    # clear data directory\n",
    "    if os.path.exists(separated_dir):\n",
    "        shutil.rmtree(separated_dir)\n",
    "        os.makedirs(separated_dir)\n",
    "    else:\n",
    "        os.makedirs(separated_dir)\n",
    "        \n",
    "    # Open the file with read only permit\n",
    "    jsonFile = io.open(input_path,'r', encoding='utf8')\n",
    "    # use readline() to read the first line \n",
    "    line = jsonFile.readline()\n",
    "    # use the read line to read further.\n",
    "    # If the file is not empty keep reading one line\n",
    "    # at a time, till the file is empty\n",
    "    ID = 0\n",
    "    flag = 0\n",
    "    empty_count = 0\n",
    "    while line:\n",
    "        record = json.loads(line)\n",
    "        abstract =  record['introduction']\n",
    "        body = record['textbody']\n",
    "        len_abs = len(abstract)\n",
    "        len_body = len(body)\n",
    "        if len_abs < 50:\n",
    "            line = jsonFile.readline()\n",
    "            continue\n",
    "        if len_body < 1000:\n",
    "            line = jsonFile.readline()\n",
    "            continue\n",
    "        if len_body/len_abs < 3:\n",
    "            line = jsonFile.readline()\n",
    "            continue\n",
    "        #write abstract\n",
    "        if not os.path.isdir(separated_dir):\n",
    "            os.mkdir(separated_dir)\n",
    "        f = open(separated_dir+str(ID)+\".abs\",\"wb\")\n",
    "        f.write(abstract.encode(\"utf-8\",'ignore'))\n",
    "        f.close()\n",
    "        #write bodytext\n",
    "        f = open(separated_dir+str(ID)+\".body\",\"wb\")\n",
    "        f.write(body.encode(\"utf-8\",'ignore'))\n",
    "        f.close()\n",
    "\n",
    "        #next line\n",
    "        ID = ID+1\n",
    "        line = jsonFile.readline()\n",
    "    jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_art_abs(input_dir,ID):\n",
    "    with open(input_dir+ID+\".abs\", 'r') as absFile:\n",
    "        abstract = absFile.read()\n",
    "    with open(input_dir+ID+\".body\", 'r') as bodyFile:\n",
    "        body = bodyFile.read()\n",
    "    if body and abstract:\n",
    "        return body,abstract\n",
    "    else:\n",
    "        exit(0)\n",
    "\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "\n",
    "def wrap_sentence_token(sent):\n",
    "    return SENTENCE_START+sent+SENTENCE_END\n",
    "\n",
    "def get_art_abs_with_st(input_dir,ID):\n",
    "    with open(input_dir+ID+\".abs\", 'r') as absFile:\n",
    "        abstract = absFile.read()\n",
    "    with open(input_dir+ID+\".body\", 'r') as bodyFile:\n",
    "        body = bodyFile.read()\n",
    "    if body and abstract:\n",
    "        sent_tokenized_list = sent_tokenize(abstract.decode(\"ascii\",errors=\"ignore\").encode())\n",
    "        sent_tokenized_list = map(wrap_sentence_token,sent_tokenized_list)\n",
    "        st_abstract = \"\".join(sent_tokenized_list)\n",
    "        #print st_abstract\n",
    "        return body,st_abstract\n",
    "    else:\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to bin file\n",
    "dm_single_close_quote = u'\\u2019' # unicode\n",
    "dm_double_close_quote = u'\\u201d'\n",
    "END_TOKENS = ['.', '!', '?', '...', \"'\", \"`\", '\"', dm_single_close_quote, dm_double_close_quote, \")\"] # acceptable ways to end a sentence\n",
    "VOCAB_SIZE = 200000\n",
    "# We use these to separate the summary sentences in the .bin datafiles\n",
    "SENTENCE_START = '<s>'\n",
    "SENTENCE_END = '</s>'\n",
    "def write_to_bin(input_dir, out_dir, makevocab=False):\n",
    "    \n",
    "    # clear data directory\n",
    "    if os.path.exists(out_dir):\n",
    "        shutil.rmtree(out_dir)\n",
    "        os.makedirs(out_dir)\n",
    "    else:\n",
    "        os.makedirs(out_dir)\n",
    "    out_file = os.path.join(out_dir, \"train.bin\")\n",
    "    \"\"\"Reads the tokenized .story files corresponding to the urls listed in the url_file and writes them to a out_file.\"\"\"\n",
    "    if makevocab:\n",
    "        vocab_counter = collections.Counter()\n",
    "    ID = 0\n",
    "    with open(out_file, 'wb') as writer:\n",
    "        # Get the strings to write to .bin file\n",
    "        num_files = len(os.listdir(input_dir))/2-1\n",
    "        for ID in range(0,num_files):\n",
    "            \n",
    "            article, abstract = get_art_abs_with_st(input_dir,str(ID))\n",
    "            #print abstract\n",
    "            # Write to tf.Example\n",
    "            tf_example = example_pb2.Example()\n",
    "            tf_example.features.feature['article'].bytes_list.value.extend([article])\n",
    "            tf_example.features.feature['abstract'].bytes_list.value.extend([abstract])\n",
    "            tf_example_str = tf_example.SerializeToString()\n",
    "            str_len = len(tf_example_str)\n",
    "            if not str_len:\n",
    "                print \"MyError-0\"\n",
    "            writer.write(struct.pack('q', str_len))\n",
    "            writer.write(struct.pack('%ds' % str_len, tf_example_str))\n",
    "\n",
    "            # Write the vocab to file, if applicable\n",
    "            if makevocab:\n",
    "                art_tokens = article.split(' ')\n",
    "                abs_tokens = abstract.split(' ')\n",
    "                abs_tokens = [t for t in abs_tokens if t not in [SENTENCE_START, SENTENCE_END]] # remove these tags from vocab\n",
    "                tokens = art_tokens + abs_tokens\n",
    "                tokens = [t.strip() for t in tokens] # strip\n",
    "                tokens = [t for t in tokens if t!=\"\"] # remove empty\n",
    "                vocab_counter.update(tokens)\n",
    "\n",
    "    print \"Finished writing file %s\\n\" % out_file\n",
    "\n",
    "    # write vocab to file\n",
    "    if makevocab:\n",
    "        print \"Writing vocab file...\"\n",
    "        with open(os.path.join(out_dir, \"vocab\"), 'w+') as writer:\n",
    "            for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "                writer.write(word + ' ' + str(count) + '\\n')\n",
    "        print \"Finished writing vocab file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n",
    "def chunk_file(input_dir,set_name):\n",
    "    \n",
    "    in_file = input_dir + '%s.bin' % set_name\n",
    "    print \"In File:\" + in_file\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "        \n",
    "def chunk_all(input_dir, chunks_dir):\n",
    "  # Make a dir to hold the chunks\n",
    "    if os.path.exists(chunks_dir):\n",
    "        shutil.rmtree(chunks_dir)\n",
    "        os.makedirs(chunks_dir)\n",
    "    else:\n",
    "        os.makedirs(chunks_dir)\n",
    "  # Chunk the data\n",
    "    for set_name in ['train']:\n",
    "        print \"Splitting %s data into chunks...\" % set_name\n",
    "        chunk_file(input_dir,set_name)\n",
    "    print \"Saved chunked data in %s\" % chunks_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing file /mnt/6t/DeepLearningPreTrainModels/wiki_thesis_st/finished_files/train.bin\n",
      "\n",
      "Writing vocab file...\n",
      "Finished writing vocab file\n",
      "Splitting train data into chunks...\n",
      "In File:/mnt/6t/DeepLearningPreTrainModels/wiki_thesis_st/finished_files/train.bin\n",
      "Saved chunked data in /mnt/6t/DeepLearningPreTrainModels/wiki_thesis_st/finished_files/chunked/\n"
     ]
    }
   ],
   "source": [
    "input_path = \"/mnt/6t/wiki/processed/Th_related_wiki_gensim_v5.json\"\n",
    "separated_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_thesis_st/separated_files/\"\n",
    "tokenized_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_thesis_st/tokenized_separated_files/\"\n",
    "output_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_thesis_st/finished_files/\"\n",
    "chunks_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_thesis_st/finished_files/chunked/\"\n",
    "    \n",
    "# separate_json(input_path,separated_dir)\n",
    "\n",
    "# tokenize_stories(separated_dir,tokenized_dir)\n",
    "\n",
    "write_to_bin(tokenized_dir,output_dir, makevocab=True)\n",
    "\n",
    "chunk_all(output_dir,chunks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/mnt/6t/wiki/processed/All_related_wiki_gensim_v5.0.json\"\n",
    "separated_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_all_st/separated_files/\"\n",
    "tokenized_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_all_st/tokenized_separated_files/\"\n",
    "output_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_all_st/finished_files/\"\n",
    "chunks_dir = \"/mnt/6t/DeepLearningPreTrainModels/wiki_all_st/finished_files/chunked/\"\n",
    "    \n",
    "separate_json(input_path,separated_dir)\n",
    "\n",
    "tokenize_stories(separated_dir,tokenized_dir)\n",
    "\n",
    "write_to_bin(tokenized_dir,output_dir, makevocab=True)\n",
    "\n",
    "chunk_all(output_dir,chunks_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch03-p27]",
   "language": "python",
   "name": "conda-env-pytorch03-p27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
